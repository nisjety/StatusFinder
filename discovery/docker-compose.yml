services:
  scrapyd:
    build:
      context: .
      dockerfile: Dockerfile
    image: discovery-scrapyd
    container_name: discovery-scrapyd
    ports:
      - "6800:6800"
    volumes:
      - ./data:/app/data
      - ./discovery:/app/discovery  # Mount source code for development
      - ./screenshots:/app/screenshots  # Dedicated screenshots directory
      - scrapyd_eggs:/app/eggs
      - scrapyd_items:/app/items
      - scrapyd_logs:/app/logs
      - scrapyd_dbs:/app/dbs
    restart: unless-stopped
    environment:
      - DISCOVERY_ENV=production
      - SCRAPYD_USERNAME=${SCRAPYD_USERNAME:-admin}
      - SCRAPYD_PASSWORD=${SCRAPYD_PASSWORD:-scrapyd}
      - PYTHONPATH=/app
      - SCRAPY_SETTINGS_MODULE=discovery.settings
      # Deployment settings
      - DEPLOY_PROJECT=discovery
      - DEPLOY_VERSION=1.0
      - DEPLOY_SPIDERS=true
    healthcheck:
      test: ["CMD", "curl", "-f", "-u", "${SCRAPYD_USERNAME}:${SCRAPYD_PASSWORD}", "http://localhost:6800/daemonstatus.json"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s
    networks:
      - discovery-network
    deploy:
      resources:
        limits:
          cpus: '2'
          memory: 2G
        reservations:
          cpus: '0.5'
          memory: 512M
  
  webhook:
    build:
      context: .
      dockerfile: Dockerfile.webhook
    image: discovery-webhook
    container_name: discovery-webhook
    ports:
      - "8000:8000"
    depends_on:
      - scrapyd
    restart: unless-stopped
    environment:
      - SCRAPYD_URL=http://scrapyd:6800
      - PROJECT_NAME=Discovery
      - DEFAULT_SPIDER=quick
    volumes:
      - ./webhook_handler.py:/app/webhook_handler.py
      - ./data/logs:/app/logs
    networks:
      - discovery-network

networks:
  discovery-network:
    driver: bridge

volumes:
  scrapyd_eggs:
  scrapyd_items:
  scrapyd_logs:
  scrapyd_dbs:
